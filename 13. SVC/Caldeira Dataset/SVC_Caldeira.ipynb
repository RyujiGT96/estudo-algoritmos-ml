{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Clustering (SVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De forma a demonstrar a aplicabilidade do SVC para problemas com dados reais, é proposto a análise de dados de operação de um processo Kraft presente em uma indústria de celulose real. Os dados a serem analisados consistem em dados de monitoramento de uma caldeira utilizada para queima de um licor proveniente da extração de celulose da madeira. Propõe-se o uso do SVC para o agrupamento de dados da caldeira em estado limpo e de dados da caldeira com depósito de sólidos. Para isso dados de operação em ambas as situações são fornecidos. Pelo fato de já se saber qual o estado da caldeira em cada amostra de dado, o problema elaborado pode ser considerado um caso de aprendizado semi-supervisionado, onde os dados são agrupados em duas classes já conhecidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliotecas a serem utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cvxpy as cvx\n",
    "from tqdm import tqdm\n",
    "import sklearn.datasets\n",
    "import time\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo Support Vector Clustering (SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cvxpy as cvx\n",
    "from tqdm import tqdm\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "class SupportVectorClustering():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def dataset(self, xs, xs_labels):\n",
    "        self.xs = xs # dataset\n",
    "        self.xs_labels = xs_labels\n",
    "        self.N = len(xs) # number de instâncias\n",
    "\n",
    "    def parameters(self, p=0.1, q=1):\n",
    "        self.q = q # parâmetro kernel width \n",
    "        self.p = p # fração de bounded support vectors (BSVs) \n",
    "        self.C = 1/(self.N*p) # constante de penalização (1/C >= 1)\n",
    "    \n",
    "    def kernel(self, x1, x2):\n",
    "        return np.exp(-self.q*np.sum((x1-x2)**2, axis=-1)) # gaussian kernel \n",
    "\n",
    "    def kernel_matrix(self):\n",
    "        self.km = np.zeros((self.N, self.N))\n",
    "        for i in range(self.N):\n",
    "            for j in range(self.N):\n",
    "                self.km[i,j] = self.kernel(self.xs[i], self.xs[j])\n",
    "\n",
    "    # método de otimização\n",
    "    def find_beta(self): \n",
    "        beta = cvx.Variable(self.N) # vetor de N dimensões\n",
    "        objective = cvx.Maximize(cvx.sum(beta) - cvx.quad_form(beta, self.km))  # função objetivo \n",
    "        constraints = [0 <= beta, beta <= self.C, cvx.sum(beta) == 1] # restrições \n",
    "        prob = cvx.Problem(objective, constraints) # definição do problema de otimização\n",
    "        prob.solve() # solução do problema de otimização\n",
    "        self.beta = beta.value # valores ótimos das variáveis beta \n",
    "\n",
    "    # cálculo do raio da hiperesfera\n",
    "    def r_func(self, x):\n",
    "        return self.kernel(x, x) - 2*np.sum([self.beta[i]*self.kernel(self.xs[i],x) for i in range(self.N)]) + self.beta.T@self.km@self.beta\n",
    "        # python > 3.5 @ matrix multiplication\n",
    "\n",
    "    # amostragem de segmentos entre dois pontos\n",
    "    def sample_segment(self,x1,x2,r,n=10):\n",
    "        adj = True\n",
    "        for i in range(n):\n",
    "            x = x1 + (x2-x1)*i/(n+1) \n",
    "            if self.r_func(x) > r:\n",
    "                adj = False\n",
    "                return adj\n",
    "        return adj\n",
    "    \n",
    "    # definição da matriz de adjacência\n",
    "    def cluster(self):\n",
    "        print('Calculating adjacency matrix... \\n')\n",
    "        svs_tmp = np.array(self.beta < self.C)*np.array(self.beta > 10**-8) # svs: 0 < beta < C\n",
    "        self.svs = np.where(svs_tmp == True)[0] # índice support vectors \n",
    "        bsvs_tmp = np.array(self.beta >= self.C) # bsvs: beta == C ??\n",
    "        self.bsvs = np.where(bsvs_tmp == True)[0] # índice bounded support vectors\n",
    "        self.r = np.mean([self.r_func(self.xs[i]) for i in self.svs[:5]]) # why 5??\n",
    "        self.adj = np.zeros((self.N, self.N)) # matriz adjacência\n",
    "        # checar adjacência entre pontos \n",
    "        for i in tqdm(range(self.N)):\n",
    "            if i not in self.bsvs:\n",
    "                for j in range(i, self.N):\n",
    "                    if j not in self.bsvs:\n",
    "                        self.adj[i,j] = self.adj[j,i] = self.sample_segment(self.xs[i],self.xs[j],self.r)\n",
    "    \n",
    "    # definição dos clusters\n",
    "    def return_clusters(self):\n",
    "        ids = list(range(self.N))\n",
    "        self.clusters = {}\n",
    "        num_clusters = 0\n",
    "        while ids:\n",
    "            num_clusters += 1\n",
    "            self.clusters[num_clusters] = []\n",
    "            curr_id = ids.pop(0)\n",
    "            queue = [curr_id]\n",
    "            while queue:\n",
    "                cid = queue.pop(0)\n",
    "                for i in ids:\n",
    "                    if self.adj[i,cid]:\n",
    "                        queue.append(i)\n",
    "                        ids.remove(i)\n",
    "                self.clusters[num_clusters].append(cid)\n",
    "        print('\\n')\n",
    "        print(f'The number of clusters is {num_clusters}')\n",
    "    \n",
    "    # resultados diagnóstico\n",
    "    def results(self):\n",
    "        clusters_results = np.zeros((len(self.clusters.keys()), 2))\n",
    "        for i in self.clusters.keys():\n",
    "            normal = 0\n",
    "            fault = 0\n",
    "            for j in self.clusters[i]:\n",
    "                if self.xs_labels[j] == 0:\n",
    "                    normal += 1\n",
    "                else:\n",
    "                    fault += 1   \n",
    "            clusters_results[i-1][0] = normal\n",
    "            clusters_results[i-1][1] = fault\n",
    "        print([0, 1])\n",
    "        print(clusters_results)\n",
    "        print(len(clusters_results))\n",
    "    \n",
    "    # centróides dos clusters\n",
    "    def clusters_centroids(self):\n",
    "        self.centroids = dict()\n",
    "        for key, values in self.clusters.items(): \n",
    "            sum_cluster_data = np.zeros(len(self.xs[0]))\n",
    "            for j in values:\n",
    "                sum_cluster_data += self.xs[j]\n",
    "            centroid = sum_cluster_data / len(values)\n",
    "            self.centroids[key] = centroid\n",
    "        print(self.centroids)\n",
    "\n",
    "    # teste do modelo\n",
    "    def test_clustering(self, xs_test, xs_labels_test):\n",
    "        self.xs_test = xs_test\n",
    "        self.xs_labels_test = xs_labels_test\n",
    "        clusters_similarity = np.zeros((len(xs_test),len(self.centroids.keys())))\n",
    "        for i, x in enumerate(xs_test):\n",
    "            for key, centroid in self.centroids.items():\n",
    "                clusters_similarity[i,key-1] = np.linalg.norm(x-centroid, 2)\n",
    "        cluster_assignment = np.argmin(clusters_similarity, axis = 1)\n",
    "        print(cluster_assignment)\n",
    "        \n",
    "        if len(self.centroids.keys()) == 2:\n",
    "            false_alarm = 0\n",
    "            warn_miss = 0\n",
    "            for i in range(len(self.xs_labels_test)):\n",
    "                if (cluster_assignment[i] != self.xs_labels_test[i]):\n",
    "                    if ((cluster_assignment[i] == 1) and (self.xs_labels_test[i] == 0)):\n",
    "                        false_alarm += 1\n",
    "                    else:\n",
    "                        warn_miss += 1\n",
    "            print(f'False Alarm {false_alarm*100/len(self.xs_labels_test)}%')\n",
    "            print(f'Missed Warning {warn_miss*100/len(self.xs_labels_test)}%')\n",
    "            print(f'Right Diagnostic {(len(self.xs_labels_test) - false_alarm - warn_miss)*100/len(self.xs_labels_test)}%')\n",
    "        else:\n",
    "            error = 0\n",
    "            for i in range(len(self.xs_labels_test)):\n",
    "                if cluster_assignment[i] != self.xs_labels_test[i]:\n",
    "                    error += 1\n",
    "            print(f'Right Diagnostic {(len(self.xs_labels_test) - error)*100/len(self.xs_labels_test)}%')\n",
    "            print(f'Wrong Diagnostic {(error)*100/len(self.xs_labels_test)}%')\n",
    "\n",
    "\n",
    "    def show_plot(self):\n",
    "        labels = np.zeros(self.xs.shape[0]) # number of samples\n",
    "        for i in self.clusters.keys():\n",
    "            for j in self.clusters[i]:\n",
    "                labels[j] = int(i) # cluster label for each point\n",
    "        \n",
    "        from pandas import DataFrame\n",
    "        from matplotlib import pyplot\n",
    "        df = DataFrame(dict(x=self.xs[:,0], y=self.xs[:,1], label=labels)) # table\n",
    "        colors ={1:'r',2:'b',3:'g',4:'c',5:'m',6:'y',7:'k',8:'b',9:'c'}\n",
    "        fig, ax = pyplot.subplots() \n",
    "        grouped = df.groupby('label')\n",
    "        for key, group in grouped:\n",
    "            group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importação dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('caldeira_dataset.csv')\n",
    "data = f.read()\n",
    "f.close()\n",
    "# conversão dos dados para matriz\n",
    "lines = data.split('\\n')\n",
    "num_rows = len(lines)\n",
    "num_cols = len(lines[0].split(';'))\n",
    "float_data = np.zeros((num_rows, num_cols-2))\n",
    "labels = np.zeros(len(float_data))\n",
    "for i, line in enumerate(lines):\n",
    "    values = [float(value) for value in line.split(';')[1:num_cols-1]]\n",
    "    float_data[i,:] = values\n",
    "    labels[i] = float(line[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divisão dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data = float_data[:100]\n",
    "fault_data = float_data[2882:2982]\n",
    "training_data = np.concatenate((normal_data, fault_data), axis = 0)\n",
    "\n",
    "# rótulos dos dados de treinamento\n",
    "normal_labels = labels[:100]\n",
    "fault_labels = labels[2882:2982]\n",
    "training_labels = np.concatenate((normal_labels, fault_labels), axis = 0)\n",
    "\n",
    "# dados de teste\n",
    "test_normal_data = float_data[1440:1540]\n",
    "test_fault_data = float_data[4322:4422]\n",
    "test_data = np.concatenate((test_normal_data, test_fault_data), axis = 0)\n",
    "\n",
    "# rótulos dos dados de teste\n",
    "test_normal_labels = labels[1440:1540]\n",
    "test_fault_labels = labels[4322:4422]\n",
    "test_data_labels = np.concatenate((test_normal_labels, test_fault_labels), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalização dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = training_data.mean(axis=0)\n",
    "std = training_data.std(axis=0)\n",
    "training_data -= mean\n",
    "training_data /= std\n",
    "test_data -= mean\n",
    "test_data /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating adjacency matrix... \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 200/200 [11:04<00:00,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The number of clusters is 2\n",
      "[0, 1]\n",
      "[[100.   0.]\n",
      " [  0. 100.]]\n",
      "2\n",
      "{1: array([ 0.87852056, -0.99983026,  0.8487985 ,  0.20856193, -0.3303804 ,\n",
      "       -0.67189928, -0.5794404 ,  0.99165125,  0.99701967, -0.47205518,\n",
      "       -0.92167641,  0.68981489,  0.99712042, -0.38669433, -1.        ,\n",
      "        0.9818645 , -0.0177221 , -0.99490296]), 2: array([-0.87852056,  0.99983026, -0.8487985 , -0.20856193,  0.3303804 ,\n",
      "        0.67189928,  0.5794404 , -0.99165125, -0.99701967,  0.47205518,\n",
      "        0.92167641, -0.68981489, -0.99712042,  0.38669433,  1.        ,\n",
      "       -0.9818645 ,  0.0177221 ,  0.99490296])}\n",
      "[1 1 1 1 0 1 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "False Alarm 4.0%\n",
      "Missed Warning 0.0%\n",
      "Right Diagnostic 96.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "svc = SupportVectorClustering()\n",
    "svc.dataset(training_data, training_labels) # database\n",
    "svc.parameters(p=0.005, q=0.24) # parâmetros # p = 0.005 q = 0.24 funcionou\n",
    "svc.kernel_matrix() # matriz kernel\n",
    "svc.find_beta() # solução problema de otimização\n",
    "svc.cluster() # matriz de adjacência\n",
    "svc.return_clusters() # define clusters\n",
    "svc.results()\n",
    "svc.clusters_centroids()\n",
    "svc.test_clustering(test_data, test_data_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
